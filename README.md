# LLM Orchestra Service

[ğŸ‡·ğŸ‡º ĞŸĞµÑ€ĞµĞ¹Ñ‚Ğ¸ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¸Ğ¹](README_RU.md) | [ğŸ‡ºğŸ‡¸ English (current)](README.md)

LLM Orchestra Service is a FastAPI-based web application designed to orchestrate and manage Large Language Model (LLM) tasks. The service provides a REST API for creating, managing, and monitoring LLM tasks with asynchronous processing capabilities using Celery and Redis.

## Features

- User authentication and authorization (JWT-based)
- Task management for LLM operations
- Asynchronous task processing with Celery and Redis
- PostgreSQL database integration with SQLAlchemy
- Docker containerization support
- RESTful API design
- Admin panel endpoints for user and task management
- Web-based admin UI for easy system management
- Automatic admin user creation
- Multiple LLM provider support (Ollama as default)

## Project Structure

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # Main FastAPI application
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ deps.py          # Dependencies for API endpoints
â”‚   â”‚   â”œâ”€â”€ endpoints/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ tasks.py     # Task-related endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ users.py     # User management endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ auth.py      # Authentication endpoints
â”‚   â”‚   â”‚   â””â”€â”€ admin.py     # Admin endpoints for user/task management
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py        # Application settings
â”‚   â”‚   â”œâ”€â”€ security.py      # JWT and password hashing
â”‚   â”‚   â””â”€â”€ startup.py       # Application startup logic
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ session.py       # Database session setup
â”‚   â”‚   â””â”€â”€ models.py        # Database models
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ tasks.py         # Pydantic schemas for tasks
â”‚   â”‚   â””â”€â”€ users.py         # Pydantic schemas for users
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ llm_service.py   # LLM interaction logic
â”‚   â””â”€â”€ tasks/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ celery_worker.py # Celery workers
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ admin/               # Next.js admin dashboard
â”œâ”€â”€ alembic/                 # Database migrations
â”‚   â”œâ”€â”€ versions/            # Migration scripts
â”‚   â”œâ”€â”€ env.py               # Alembic environment configuration
â”‚   â””â”€â”€ script.py.mako       # Migration script template
â”œâ”€â”€ .env.example             # Environment variables example
â”œâ”€â”€ alembic.ini              # Alembic configuration
â”œâ”€â”€ Dockerfile               # Docker configuration
â”œâ”€â”€ docker-compose.yml       # Docker Compose configuration
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ README.md                # English documentation
â””â”€â”€ README_RU.md             # Russian documentation
```

## Prerequisites

- Python 3.8+
- Node.js 18+
- PostgreSQL
- Redis
- Docker (optional, for containerization)
- Ollama (for local LLM support)

## Installation

1. Clone the repository:
   ```bash
   git clone <repository-url>
   cd llm-orchestra-service
   ```

2. Create a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. Install Python dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Install UI dependencies:
   ```bash
   cd ui/admin
   npm install  # or pnpm install
   ```

5. Set up environment variables:
   ```bash
   cp .env.example .env
   cp ui/admin/.env.example ui/admin/.env
   ```
   Edit the `.env` files with your actual configuration values.

## Database Setup

1. Make sure PostgreSQL is running and create the database:
   ```bash
   createdb llm_orchestra
   ```

2. Update the `alembic.ini` file with your database connection string:
   ```
   sqlalchemy.url = postgresql://username:password@localhost/llm_orchestra
   ```

3. Run Alembic migrations:
   ```bash
   alembic upgrade head
   ```

## LLM Provider Configuration

The service supports multiple LLM providers with Ollama as the default:

### Ollama (Default)
1. Install Ollama from https://ollama.com/
2. Pull a model (e.g., llama3):
   ```bash
   ollama pull llama3
   ```
3. Start the Ollama service:
   ```bash
   ollama serve
   ```

### OpenAI (Alternative)
1. Get an API key from OpenAI
2. Set the `OPENAI_API_KEY` environment variable
3. Set `LLM_PROVIDER=openai` in your environment

### Configuration Variables
- `LLM_PROVIDER`: The LLM provider to use (default: "ollama")
- `LLM_MODEL`: The model to use (default: "llama3" for Ollama)
- `OLLAMA_BASE_URL`: The base URL for Ollama (default: "http://localhost:11434")
- `OPENAI_API_KEY`: API key for OpenAI (only needed if using OpenAI)

## Running the Application

### Development Mode

1. Start the FastAPI server:
   ```bash
   uvicorn app.main:app --reload
   ```

2. Start the Celery worker (in a separate terminal):
   ```bash
   celery -A app.tasks.celery_worker.celery_app worker --loglevel=info
   ```

3. (Optional) Start the Celery beat scheduler for periodic tasks:
   ```bash
   celery -A app.tasks.celery_worker.celery_app beat --loglevel=info
   ```

4. Start the Admin UI (in a separate terminal):
   ```bash
   cd ui/admin
   npm run dev  # or pnpm dev
   ```

### Using Docker

1. Build and start all services:
   ```bash
   docker-compose up --build
   ```

2. The services will be available at:
   - API: `http://localhost:8000`
   - Admin UI: `http://localhost:3000`
   - Database: `http://localhost:5432`
   - Redis: `http://localhost:6379`

## API Documentation

- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

## Admin Endpoints

The service includes a comprehensive set of admin endpoints for managing users and monitoring tasks:

### Authentication
- `POST /token` - Obtain access token with username and password

### User Management
- `GET /users/me` - Get current user information
- `GET /admin/users` - Get all users (admin only)
- `POST /admin/users` - Create a new user (admin only)
- `PUT /admin/users/{user_id}` - Update user details (admin only)
- `DELETE /admin/users/{user_id}` - Delete a user (admin only)

### Task Monitoring
- `GET /admin/tasks/all` - Get all tasks in the system (admin only)
- `GET /admin/tasks/{task_id}` - Get details of a specific task (admin only)

### Statistics
- `GET /admin/stats/tasks/by_status` - Get task counts by status (admin only)
- `GET /admin/stats/tasks/by_type` - Get task counts by type (admin only)

## Admin UI

The project includes a web-based admin dashboard built with Next.js that provides a user-friendly interface for:

- Managing users (create, update, delete)
- Monitoring tasks (view status, details)
- Viewing system statistics

The UI features JWT-based authentication that integrates with the backend API. On first login, use the admin credentials configured in your environment variables.

## Testing Authentication

To test the authentication flow:

1. Start the application using either development mode or Docker
2. Navigate to `http://localhost:3000` to access the admin UI
3. Log in with the admin credentials (default: username `admin`, password `admin`)
4. If you need to test authentication programmatically, you can use the `/token` endpoint directly

## Usage

1. On first startup, an admin user will be automatically created with the credentials from environment variables
2. Navigate to `http://localhost:3000` to access the admin UI
3. Log in with the admin credentials
4. Use the admin panel to manage users and monitor tasks
5. Create additional users as needed through the admin interface

## Environment Variables

- `DATABASE_URL`: PostgreSQL database connection string
- `SECRET_KEY`: Secret key for JWT token generation
- `ALGORITHM`: Algorithm for JWT token encoding
- `ACCESS_TOKEN_EXPIRE_MINUTES`: Token expiration time
- `CELERY_BROKER_URL`: Redis URL for Celery broker
- `CELERY_RESULT_BACKEND`: Redis URL for Celery results
- `LLM_PROVIDER`: LLM provider to use (default: "ollama")
- `LLM_MODEL`: Model to use (default: "llama3")
- `OLLAMA_BASE_URL`: Base URL for Ollama service (default: "http://localhost:11434")
- `OPENAI_API_KEY`: OpenAI API key for LLM access (only needed if using OpenAI)
- `ADMIN_USERNAME`: Username for the default admin user (default: admin)
- `ADMIN_PASSWORD`: Password for the default admin user (default: admin)
- `NEXT_PUBLIC_API_URL`: API URL for the admin UI (in UI .env file)

## Common Issues and Fixes

1. **Database Connection Issues**: Ensure PostgreSQL is running and the connection string in `.env` is correct.

2. **Redis Connection Issues**: Make sure Redis is running on the specified port (default 6379).

3. **Missing Dependencies**: If you encounter import errors, make sure all dependencies from `requirements.txt` are installed.

4. **Alembic Migrations**: If you modify the database models, generate new migrations:
   ```bash
   alembic revision --autogenerate -m "Description of changes"
   alembic upgrade head
   ```

5. **LLM Provider Issues**: 
   - For Ollama: Ensure the Ollama service is running and the specified model is available
   - For OpenAI: Ensure the API key is valid and has sufficient credits

## Documentation Languages

- [English](README.md)
- [Ğ ÑƒÑÑĞºĞ¸Ğ¹](README_RU.md)

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a pull request

## License

This project is licensed under the MIT License.